{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf78246-e0e3-4567-afcc-99aa94ae7269",
   "metadata": {},
   "source": [
    "### Simple Transfer Demo\n",
    "This notebook is for demo of data augmentation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109bd94-3ec5-45a7-99b7-e920ebafda91",
   "metadata": {},
   "source": [
    "### 1. Load necessary Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3765e8-b688-4155-9c08-59a95ff14dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from surprise import SVD, Dataset, Reader, SVDpp\n",
    "from surprise import accuracy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d7d37-c8db-4301-b9cd-7ebd6c504878",
   "metadata": {},
   "source": [
    "### 2. Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df84edd-7300-4e5a-a030-13d521e4450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML1M datasets\n",
    "\n",
    "movies_df = pd.read_csv('../datasets/ml-1m/movies.dat', sep='::', names=['item_id', 'title', 'genres'], engine = \"python\", encoding = \"ISO-8859-1\")\n",
    "movies_df['item_id'] = movies_df['item_id'].astype(np.int)\n",
    "\n",
    "ratings_df = pd.read_csv('../datasets/ml-1m/ratings.dat', sep='::', names=['user_id', 'item_id', 'rating', 'timestamp'], engine = \"python\")\n",
    "ratings_df['user_id'] = ratings_df['user_id'].astype(np.int)\n",
    "ratings_df['item_id'] = ratings_df['item_id'].astype(np.int)\n",
    "data_df_1m = pd.merge(ratings_df, movies_df, how = \"inner\", on = \"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0f6db7-8d1c-4c7f-83b5-a33299242914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML100K\n",
    "\n",
    "ratings_df_100k = pd.read_csv(\"../datasets/ml-100k/u.data\", \n",
    "                         sep = \"\\t\",\n",
    "                         names = ['user_id', 'item_id', 'rating', 'timestamp'], \n",
    "                         engine = \"python\")\n",
    "\n",
    "m_cols = [\"item_id\", \"title\"]\n",
    "movies_df_100k = pd.read_csv('../datasets/ml-100k/u.item',\n",
    "                        names = m_cols,\n",
    "                        usecols = range(2),\n",
    "                        sep='|', \n",
    "                        engine = \"python\",\n",
    "                        encoding = \"ISO-8859-1\")\n",
    "\n",
    "data_df_100 = pd.merge(ratings_df_100k, movies_df_100k, how = \"inner\", on = \"item_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a33318-3e9a-45fa-9f5b-af3c5eae030c",
   "metadata": {},
   "source": [
    "### 3.1 Encode item cross two domains (titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78bfa913-1429-4335-99a5-0b2371090441",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_100k = data_df_100[\"title\"].values\n",
    "arr_1m = data_df_1m[\"title\"].values\n",
    "all_titles = np.concatenate((arr_1m, arr_100k))\n",
    "all_unique = np.unique(all_titles)\n",
    "\n",
    "\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(all_unique)\n",
    "\n",
    "data_df_100[\"item_id\"] = lbe.transform(data_df_100[\"title\"])\n",
    "data_df_1m[\"item_id\"] = lbe.transform(data_df_1m[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fb0901-2460-40d7-8d1b-4011cd6aa4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique item in ml100k: 1664, number of unique item in ml1m: 3706\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of unique item in ml100k: {data_df_100['item_id'].nunique()}, number of unique item in ml1m: {data_df_1m['item_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972a8fd-fdb4-43c9-95ce-4fdef9334c90",
   "metadata": {},
   "source": [
    "### 3.2 Encode user cross two domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221f9df9-5c52-4b2b-afad-c504c4fc9bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique user in ml100k: 943\n"
     ]
    }
   ],
   "source": [
    "lbe = LabelEncoder()\n",
    "data_df_100[\"user_id\"] = lbe.fit_transform(data_df_100[\"user_id\"])\n",
    "print(f\"number of unique user in ml100k: {data_df_100['user_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70474744-cfa8-412d-a527-38e08e5935db",
   "metadata": {},
   "source": [
    "### 3.3 rename the user id: The first 943(0--942) users are from ml100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c1f854-652d-483d-bcc2-4d48d7f682cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe = LabelEncoder()\n",
    "data_df_1m[\"user_id\"] = lbe.fit_transform(data_df_1m[\"user_id\"])\n",
    "data_df_1m[\"user_id\"] = data_df_1m[\"user_id\"] + 943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7283e919-13cc-4deb-9caf-8fce7ef912a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_1m[\"user_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40bd933-c56a-4d33-9a8e-b1abaa140c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml1m interaction: 1000209, ml100k interactions: 100000, ml1m density: 0.044683625622312845\n"
     ]
    }
   ],
   "source": [
    "print(f\"ml1m interaction: {data_df_1m.shape[0]}, ml100k interactions: {data_df_100.shape[0]}, ml1m density: {data_df_1m.shape[0] / (data_df_1m['user_id'].nunique() * data_df_1m['item_id'].nunique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54316424-5948-4829-b940-fb18a34659f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml100k density: 0.06372868912635615\n"
     ]
    }
   ],
   "source": [
    "print(f\"ml100k density: {data_df_100.shape[0] / (data_df_100['user_id'].nunique() * data_df_100['item_id'].nunique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb75be-77dd-41e8-88cb-10de8283438b",
   "metadata": {},
   "source": [
    "### 4. train, test, valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48b9823-aa39-4df3-8114-71b6ddfe15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 1m\n",
    "train_df_1m, rest_df_1m = train_test_split(data_df_1m, test_size = 0.2, random_state = 133)\n",
    "test_df_1m, valid_df_1m = train_test_split(rest_df_1m , test_size = 0.5, random_state = 133)\n",
    "# 100k\n",
    "train_df_100, rest_df_100 = train_test_split(data_df_100, test_size = 0.2, random_state = 133)\n",
    "test_df_100, valid_df_100 = train_test_split(rest_df_100, test_size = 0.5, random_state = 133)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630a5140-a542-40f8-9c2c-3201471182f3",
   "metadata": {},
   "source": [
    "### 5. Augment 2 training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d845508-d290-4dca-9091-fbdef2e0e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_100.drop(columns= [\"title\"], inplace = True)\n",
    "train_df_1m = train_df_1m.drop(columns = [\"title\", \"genres\"])\n",
    "train_df = pd.concat([train_df_100, train_df_1m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "358840cd-3444-4e70-8567-3406f765a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "the rmse on ml100k test data if we concatenate datasets together:0.9141560790060519\n"
     ]
    }
   ],
   "source": [
    "random.seed(133)\n",
    "np.random.seed(133)\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "training_data = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
    "testing_data = Dataset.load_from_df(test_df_100[['user_id', 'item_id', 'rating']], reader)\n",
    "algo = SVD(verbose = True)\n",
    "training_data = training_data.build_full_trainset()\n",
    "testing_data = testing_data.build_full_trainset().build_testset()\n",
    "\n",
    "algo.fit(training_data,)\n",
    "training_eval = training_data.build_testset()\n",
    "train_pre = algo.test(training_eval)\n",
    "train_rmse = accuracy.rmse(train_pre, verbose=False)\n",
    "test_pre = algo.test(testing_data)\n",
    "test_rmse = accuracy.rmse(test_pre, verbose=False)\n",
    "\n",
    "print(f\"the rmse on ml100k test data if we concatenate datasets together:{test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc136f6-8e04-411e-bdc9-38f008e08f54",
   "metadata": {},
   "source": [
    "### 6. Test on ml100k alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bbda862-4d83-4ee3-bb90-52b0171b8350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "the rmse on test if we train ml100k alone:0.9337442712215435\n"
     ]
    }
   ],
   "source": [
    "random.seed(133)\n",
    "np.random.seed(133)\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "training_data = Dataset.load_from_df(train_df_100[['user_id', 'item_id', 'rating']], reader)\n",
    "testing_data = Dataset.load_from_df(test_df_100[['user_id', 'item_id', 'rating']], reader)\n",
    "algo = SVD(verbose = True)\n",
    "training_data = training_data.build_full_trainset()\n",
    "testing_data = testing_data.build_full_trainset().build_testset()\n",
    "\n",
    "algo.fit(training_data,)\n",
    "training_eval = training_data.build_testset()\n",
    "train_pre = algo.test(training_eval)\n",
    "train_rmse = accuracy.rmse(train_pre, verbose=False)\n",
    "test_pre = algo.test(testing_data)\n",
    "test_rmse = accuracy.rmse(test_pre, verbose=False)\n",
    "\n",
    "print(f\"the rmse on test if we train ml100k alone:{test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a4c5f-8460-476a-8fb7-6cbdd9599281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
